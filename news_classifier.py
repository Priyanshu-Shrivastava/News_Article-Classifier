# -*- coding: utf-8 -*-
"""Untitled1.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1x8WsYV4Fnt3-hhTz_ADrpeKkRqp3a0lS
"""

from google.colab import files
uploaded = files.upload()

import pandas as pd
train = pd.read_csv("BBC News Train.csv")

train.shape

test = pd.read_csv("BBC News Test.csv")
#sample = pd.read_csv("BBC News Sample Solution.csv")

test = pd.merge(test, sample, on='ArticleId', how='outer')

test.head(5)
#test.drop(columns = ['ArticleId'], inplace = True)

import matplotlib.pyplot as plt
train.head(5)

from collections import Counter

countz = Counter(train['Category'])

labels, values = zip(*countz.items())

plt.bar(labels,values)

from sklearn.feature_extraction.text import TfidfVectorizer

tfidf = TfidfVectorizer(sublinear_tf=True, min_df=5, norm='l2', encoding='latin-1', ngram_range=(1, 2), stop_words='english', max_features = 500)

features = tfidf.fit_transform(train['Text']).toarray()

features_test = tfidf.transform(test['Text']).toarray()

features.shape

train.shape

from sklearn.manifold import TSNE
import numpy as np

# Sampling a subset of our dataset because t-SNE is computationally expensive
SAMPLE_SIZE = int(len(features) * 0.3)
np.random.seed(0)
indices = np.random.choice(range(len(features)), size=SAMPLE_SIZE, replace=False)          # Randomly select 30 % of samples
projected_features = TSNE(n_components=2, random_state=0).fit_transform(features[indices])

train['category_id'] = train['Category'].factorize()[0]

test['category_id'] = test['Category'].factorize()[0]

category_id_df = train[['Category', 'category_id']].drop_duplicates().sort_values('category_id')

category_id_df

category_to_id = dict(category_id_df.values)
id_to_category = dict(category_id_df[['category_id', 'Category']].values)

labels = train.category_id

labels.shape

labels_text = test.category_id

projected_features[(labels[indices] == 0).values][0][0]

colors = ['pink', 'green', 'midnightblue', 'orange', 'darkgrey']

# Find points belonging to each category and plot them
for category, category_id in sorted(category_to_id.items()):
    points = projected_features[(labels[indices] == category_id).values]
    plt.scatter(points[:, 0], points[:, 1], s=30, c=colors[category_id], label=category)
plt.title("tf-idf feature vector for each article, projected on 2 dimensions.",
          fontdict=dict(fontsize=15))
plt.legend()

from sklearn.linear_model import LogisticRegression
from sklearn.ensemble import RandomForestClassifier
from sklearn.naive_bayes import MultinomialNB
from sklearn.svm import SVC 

from sklearn.model_selection import cross_val_score

models = [
    #RandomForestClassifier(n_estimators=200, max_depth=3, random_state=0),
    MultinomialNB(),
    LogisticRegression(random_state=0),
    SVC(kernel= 'rbf')
]

CV = 6  # Cross Validate with 5 different folds of 20% data ( 80-20 split with 5 folds )

#Create a data frame that will store the results for all 5 trials of the 3 different models
cv_df = pd.DataFrame(index=range(CV * len(models)))
entries = [] # Initially all entries are empty

for model in models:
  model_name = model.__class__.__name__
  # create 5 models with different 20% test sets, and store their accuracies
  accuracies = cross_val_score(model, features, labels, scoring='accuracy', cv=CV)
  # Append all 5 accuracies into the entries list ( after all 3 models are run, there will be 3x5 = 15 entries)
  for fold_idx, accuracy in enumerate(accuracies):
    entries.append((model_name, fold_idx, accuracy))

cv_df = pd.DataFrame(entries, columns=['model_name', 'fold_idx', 'accuracy'])

cv_df.groupby(by = 'model_name').mean()

cv_df

from sklearn.model_selection import train_test_split
X_train,X_test,y_train,y_test = train_test_split(features, labels, test_size = 0.33)
model = MultinomialNB()
model.fit(X_train,y_train)
y_pred = model.predict(X_test)
#y_pred_prob = model.predict_proba(X_test)

from sklearn.metrics import classification_report

target_names = category_id_df['Category'].values
print(classification_report(y_test, y_pred, target_names=target_names))

category_id_df['Category'].values

y_pred = model.predict(X_test)

from sklearn.metrics import accuracy_score

accuracy_score(y_test, y_pred)

labels.shape

model = LogisticRegression(random_state = 0)
model.fit(features,labels)

y_pred = model.predict(features_test)

y_pred.shape

articleId = test['ArticleId']

Y_pred_name =[]
for cat_id in y_pred :
    Y_pred_name.append(id_to_category[cat_id])

df = pd.DataFrame({'aid' : articleId,'pred' : Y_pred_name} )

df.to_csv('prednm.csv')

from google.colab import files
uploaded = files.upload()

sample = pd.read_csv("BBC News Sample Solution.csv")

y_true = []
for cat in sample['Category']:
  y_true.append(category_to_id[cat])

accuracy_score(y_true,y_pred)

from sklearn.svm import SVC

clf = SVC(kernel = 'rbf')
clf.fit(features,labels)

accuracy_score(y_test, y_predd)

y_predd = clf.predict(X_test)

from sklearn.feature_extraction import text
mysw = text.ENGLISH_STOP_WORDS

from sklearn.metrics import confusion_matrix
import matplotlib.pyplot as plt

import seaborn as sns

conf_mat = confusion_matrix(y_test, y_pred)
sns.heatmap(conf_mat, annot=True, fmt='d',
            xticklabels=category_id_df.Category.values, yticklabels=category_id_df.Category.values, cmap = 'Blues')
plt.ylabel('TRUE')
plt.xlabel('PRED')

import numpy as np
import matplotlib.pyplot as plt
from sklearn import svm, datasets
from sklearn.metrics import roc_curve, auc
from sklearn.preprocessing import label_binarize
from sklearn.multiclass import OneVsRestClassifier
from sklearn.linear_model import LogisticRegression


# Binarize the output
y = label_binarize(train['category_id'], classes=[0, 1, 2,3,4])
n_classes = y.shape[1]

# shuffle and split training and test sets
X_train, X_test, y_train, y_test = train_test_split(features, y, test_size=.33,
                                                    random_state=0)

# Learn to predict each class against the other
classifier = OneVsRestClassifier(LogisticRegression(random_state=0))
y_score = classifier.fit(X_train, y_train).decision_function(X_test)

# Compute ROC curve and ROC area for each class
fpr = dict()
tpr = dict()
roc_auc = dict()
for i in range(n_classes):
    fpr[i], tpr[i], _ = roc_curve(y_test[:, i], y_score[:, i])
    roc_auc[i] = auc(fpr[i], tpr[i])

plt.figure(figsize=(8, ))
for i in range(n_classes):
    plt.plot(fpr[i], tpr[i], label='ROC curve of {0} (area = {1:0.9f})'
                                   ''.format(category_id_df[category_id_df['category_id'] == i].Category, roc_auc[i]))
plt.plot([0, 1], [0, 1], 'k--')
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.legend(loc="lower right")
plt.show()

pip install pygooglenews

from pygooglenews import GoogleNews
# default GoogleNews instance
gn = GoogleNews(lang = 'en', country = 'US')

business = gn.topic_headlines('BUSINESS', proxies=None, scraping_bee = None)

pip install newsapi-python

from newsapi import NewsApiClient

api = NewsApiClient(api_key='b1b6e4269a814e8bbe8a477b3226e918')

business = api.get_top_headlines(category = 'business', language = 'en', country='us')

sport = api.get_top_headlines(category = 'sports', language = 'en', country='us')

enter = api.get_top_headlines(category = 'entertainment', language = 'en', country='us')

tech = api.get_top_headlines(category = 'technology', language = 'en', country='us')

import pandas as pd
cat = []
desc = []
for i in range(0,len(business['articles'])):
  cat.append('business')
  desc.append(business['articles'][i]['description'])

for i in range(0,len(sport['articles'])):
  cat.append('sport')
  desc.append(sport['articles'][i]['description'])
for i in range(0,len(tech['articles'])):
  cat.append('tech')
  desc.append(tech['articles'][i]['description'])
for i in range(0,len(enter['articles'])):
  cat.append('entertainment')
  desc.append(enter['articles'][i]['description'])

category = []
description = []
for i in range(0,len(desc)):
  if desc[i] is None:
    continue
  else:
    category.append(cat[i])
    description.append(desc[i])

punctuations = '''!()-[]{};:'"\,<>./?@#$%^&*_~'''

for i in range(0,len(description)):
  no_punct = ""
  for char in description[i]:
    if char not in punctuations:
        no_punct = no_punct + char
  description[i] = no_punct

features_t = tfidf.transform(description).toarray()

y_true = []
for cat in category:
  y_true.append(category_to_id[cat])

len(y_true)

y_pred = model.predict(features_t)

accuracy_score(y_true,y_pred)

y_pred_svm = clf.predict(features_t)

accuracy_score(y_true,y_pred_svm)

ndf = pd.DataFrame({'category': category, 'article':description})

from google.colab import files
ndf.to_csv('unseen.csv') 
files.download('unseen.csv')